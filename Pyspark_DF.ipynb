{"cells":[{"cell_type":"code","source":["from pyspark import *\nfrom pyspark.sql import *\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\n\n#Read input csv file\n\npath = \"/FileStore/tables/electronic_card_transactions_may_2021_csv_tables.csv\"\ndf2 = spark.read.format(\"csv\").option(\"header\",\"true\").option('inferschema','true')\\\n  .option(\"delimiter\", \",\").option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\")\\\n    .option(\"multiline\", \"true\").load(path)\n\ndf_trimble.write.option(\"header\",\"true\").mode(\"overwrite\").option(\"inferSchema\",\"true\").option(\"delimiter\", \",\").option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\")\\\n      .option(\"multiline\", \"true\").parquet(\"/mnt/curated/trimble/\"+yesterday_name)\n\n#Copying one to another dataframe\ntransactions = df2\n\n#Example for lazy evolution\ntransactions.show()\n\ndataframe.select('STATUS').distinct().show()\n\n#Selecting specific columns\ntnx1 = transactions.select('Series_reference','Period','Data_Value','STATUS')\ntnx1.count()\n\n#Data filtering \nl = ['C','F']\ntnx2 = tnx1.filter(tnx1.STATUS.isin(l))\ntnx2.count()\n\n#windowing function for aggregation\nw = Window().partitionBy(\"Series_reference\",\"Period\").orderBy(col(\"Period\").desc())\ntnx3 = tnx2.withColumn(\"max_Data_value\", max('Data_Value').over(w))\ntxn4 = tnx3.select('Series_reference','Period','max_Data_Value','STATUS')\n\n\nthreshold = sc.parallelize([(\"2001.03\", \"2000\"), (\"2002.03\", \"2000\"),\n(\"2003.03\", \"2000\"), (\"2004.03\", \"2000\"),\n(\"2005.03\", \"4000\"), (\"2005.03\", \"5000\"),\n(\"2007.03\", \"4000\"), (\"2008.03\", \"5000\"),\n(\"2009.03\", \"4000\"), (\"2010.03\", \"5000\"),\n(\"2011.03\", \"6000\"), (\"2012.03\", \"12000\"),\n(\"2013.03\", \"6000\"), (\"2014.03\", \"12000\"),\n(\"2015.03\", \"6000\"), (\"2016.03\", \"12000\"),\n(\"2017.03\", \"8000\"), (\"2018.03\", \"9000\"),\n(\"2019.03\", \"8000\"), (\"2002.03\", \"9000\"),\n(\"2021.03\", \"8000\")])\n\n#create RDD, convert RDD to DF\nthreshold = spark.createDataFrame(threshold).toDF(\"tsh_Period\", \"tsh_Value\")\nthreshold.display()\n\n#Join 2 dataframes\nsdf_txn_tsh = txn4.alias('txn')\\\n                                .join(threshold.alias('threshold'),(col('txn.Period') == col('threshold.tsh_Period')), how=\"left\")\\\n.select('Series_reference','Period','max_Data_Value','STATUS','tsh_Value')\n\n#isNotnull, withcolumn, When, withcolumnRenamed, type casting\n\n\n# sdf_txn_tsh.show()\nsdf_txn_tsh = sdf_txn_tsh.filter(sdf_txn_tsh.max_Data_Value.isNotNull())\nsdf_txn_tsh.show()\n\n\n# sdf_txn_tsh_1 = sdf_txn_tsh.fillna( { 'tsh_Value':0})\n# sdf_txn_tsh_1.show()\nsdf_txn_tsh_2 = sdf_txn_tsh_1.withColumn('Is_Profit',when(sdf_txn_tsh_1.max_Data_Value > sdf_txn_tsh_1.tsh_Value , 'Y').otherwise('N').cast(StringType()))\n\n\nsdf_txn_tsh_2 = sdf_txn_tsh_2.withColumnRenamed('Is_Profit','Profit?')\n\nsdf_txn_tsh_2.show()\n\n#Duplicate check\nsdf_txn_tsh_2 = sdf_txn_tsh_2.withColumn(\"Dup\", count(\"*\").over(Window.partitionBy('Series_reference','Period')))\n\n# Keep the count\nDuplicates_count = sdf_txn_tsh_2.filter(sdf_txn_tsh_2.Dup > 1).count()\n\n#Removing duplicates - usage of row number\n\nw = Window().partitionBy('Series_reference','Period').orderBy(col(\"max_Data_Value\").desc())\nsdf_txn_tsh_2 = sdf_txn_tsh_2.withColumn(\"rn\", row_number().over(w)).where(col(\"rn\") == 1).select(\"*\")\nre\n#Write back the df to any repository\n\nsdf_parq_file  = spark.read.format('parquet').option(\"header\",\"true\")\\\n  .option(\"delimiter\", \",\").option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\")\\\n    .option(\"multiline\", \"true\").load('/FileStore/tables/electronic_card_transactions_may_2021_tables')\n\nsdf_parq_file.show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5e905855-1f96-47e2-bc0a-46f1593a8f84"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["union of 3 dataframes - reduce"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ef0b208-025e-42f1-bed4-9958272fa22d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Pyspark_DF","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":724229295297457}},"nbformat":4,"nbformat_minor":0}
